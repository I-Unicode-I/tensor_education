{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCEu2koMzEHg",
        "outputId": "d237ad15-d126-42c0-9c95-3cd828428d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wget --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow==2.12.0 tensorflow_hub==0.13.0 --quiet"
      ],
      "metadata": {
        "id": "U4ckbepqpzgO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py --quiet"
      ],
      "metadata": {
        "id": "X_0gmKpHzOJe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\" --quiet"
      ],
      "metadata": {
        "id": "eUR_Khz4zSXL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import series of helper functions for the notebook\n",
        "from helper_functions import unzip_data\n",
        "\n",
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "metadata": {
        "id": "SwD1XHpuzT2p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing TensorFlow and Keras libraries\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, GlobalAveragePooling1D, Dense, LSTM, Conv1D, GlobalMaxPool1D\n",
        "from keras.layers import TextVectorization, Embedding\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "jwc3xCRtzUyb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreprocessData:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train_df = pd.read_csv('train.csv')\n",
        "        self.test_df = pd.read_csv('test.csv')\n",
        "\n",
        "        self.train_df_shuffled = self.train_df.sample(frac=1, random_state=42)\n",
        "\n",
        "        self.train_sentences = None\n",
        "        self.train_labels = None\n",
        "        self.val_sentences = None\n",
        "        self.val_labels = None\n",
        "\n",
        "        self.max_vocab_length = 10000\n",
        "        self.max_output_sequence_length = 15\n",
        "\n",
        "    def get_train_dataframe(self):\n",
        "        return self.train_df_shuffled\n",
        "\n",
        "    def get_test_dataframe(self):\n",
        "        return self.test_df\n"
      ],
      "metadata": {
        "id": "Utr6IRwZzXm4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset for this file\n",
        "preprocess_data = PreprocessData()\n",
        "\n",
        "# Read train dataset data\n",
        "train_df = preprocess_data.get_train_dataframe()\n",
        "# Read test dataset data\n",
        "test_df = preprocess_data.get_test_dataframe()\n",
        "\n",
        "# Split sentences and target (labels) from dataset\n",
        "train_sentences = train_df[\"text\"].to_numpy()\n",
        "train_labels = train_df[\"target\"].to_numpy()\n",
        "test_sentences = test_df[\"text\"].to_numpy()"
      ],
      "metadata": {
        "id": "8O9P9TaizXkj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shape=[],  # shape of input coming to our model\n",
        "                                        dtype=tf.string,  # data type of input coming to the USE layer\n",
        "                                        trainable=False,  # keep the pretrained weight\n",
        "                                                          # (we'll create a feature extractor)\n",
        "                                        name=\"USE\")"
      ],
      "metadata": {
        "id": "TCZBcuLKzXh4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model using the Sequential API\n",
        "ex_model_6 = Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_6_USE\")"
      ],
      "metadata": {
        "id": "zaN3IlIQzXfi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "ex_model_6.compile(loss=\"binary_crossentropy\",\n",
        "                    optimizer=Adam(),\n",
        "                    metrics=[\"accuracy\"])\n",
        "\n",
        "# model_6.summary()"
      ],
      "metadata": {
        "id": "0YQdbtyzzXdE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a classifier on top of pretrained embeddings\n",
        "ex_model_6_history = ex_model_6.fit(train_sentences,\n",
        "                                    train_labels,\n",
        "                                    epochs=5,\n",
        "                                    callbacks=[])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ3NW0YHzXai",
        "outputId": "fb680b06-932a-4960-9531-5f09ee317084"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "238/238 [==============================] - 10s 10ms/step - loss: 0.5017 - accuracy: 0.7796\n",
            "Epoch 2/5\n",
            "238/238 [==============================] - 2s 10ms/step - loss: 0.4140 - accuracy: 0.8151\n",
            "Epoch 3/5\n",
            "238/238 [==============================] - 2s 10ms/step - loss: 0.4014 - accuracy: 0.8240\n",
            "Epoch 4/5\n",
            "238/238 [==============================] - 3s 14ms/step - loss: 0.3930 - accuracy: 0.8292\n",
            "Epoch 5/5\n",
            "238/238 [==============================] - 2s 10ms/step - loss: 0.3849 - accuracy: 0.8311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with USE TF Hub model\n",
        "ex_model_6_pred_probs = ex_model_6.predict(test_sentences)\n",
        "# print(model_6_pred_probs[:10])\n",
        "\n",
        "# Convert prediction probabilities to labels\n",
        "ex_model_6_preds = tf.squeeze(tf.round(ex_model_6_pred_probs))\n",
        "# print(model_6_preds[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJsmcIfzXX3",
        "outputId": "26209b14-693b-41f7-d74a-56bf4da97fbd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 1s 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check predicted labels\n",
        "print(ex_model_6_preds)\n",
        "# Convert dtype in preds\n",
        "ex_model_6_preds = tf.cast(ex_model_6_preds, tf.int32)\n",
        "print(ex_model_6_preds)\n",
        "\n",
        "# Form new dataframe for this predictions\n",
        "pred_df = pd.DataFrame({\"id\": test_df[\"id\"].to_numpy(), \"target\": ex_model_6_preds.numpy()})\n",
        "\n",
        "# Check new dataframe\n",
        "print(pred_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD3Jq038zXVV",
        "outputId": "b1462e19-a8db-43ba-a9d8-3af89fa808e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1. 1. 1. ... 1. 1. 1.], shape=(3263,), dtype=float32)\n",
            "tf.Tensor([1 1 1 ... 1 1 1], shape=(3263,), dtype=int32)\n",
            "   id  target\n",
            "0   0       1\n",
            "1   2       1\n",
            "2   3       1\n",
            "3   9       1\n",
            "4  11       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Save result to csv file\n",
        "    submission_path = Path(\"output/nlp_with_tf/nlp_submission.csv\")\n",
        "    submission_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    pred_df.to_csv(submission_path, index=False)"
      ],
      "metadata": {
        "id": "B9eH2kHL2Mhg"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}